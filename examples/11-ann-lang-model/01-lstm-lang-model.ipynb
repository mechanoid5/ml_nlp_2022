{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9095d55d",
   "metadata": {},
   "source": [
    "__Нейросетевая языковая модель на основе LSTM__ \n",
    "\n",
    "Евгений Борисов <esborisov@sevsu.ru>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae74890",
   "metadata": {},
   "source": [
    "Shivam Bansal   \n",
    "Language Modelling and Text Generation using LSTMs — Deep Learning for NLP.    \n",
    "Mar 26, 2018\n",
    "\n",
    "https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82d840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465595\n"
     ]
    }
   ],
   "source": [
    "# загружаем текст\n",
    "import gzip\n",
    "with gzip.open('../data/dostoevsky-besy-p2.txt.gz','rt',encoding='utf-8') as f: data = f.read()     \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34947f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize as nltk_sentence_split\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenize_word\n",
    "\n",
    "EOS = '<EOS>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "text = [ \n",
    "    nltk_tokenize_word(s,language='russian')+[EOS] # разбиваем предложения на слова\n",
    "    for s in nltk_sentence_split(data,language='russian') # режем текст на отдельные предложения\n",
    "]\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e50445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample\n",
    "\n",
    "# def get_sample(text,min_len=10):\n",
    "#     for _ in range(100):\n",
    "#         sentence = sample(text,1)[0]\n",
    "#         # print(len(sentence),sentence)\n",
    "#         if len(sentence)>min_len:\n",
    "#             return ' '.join(sentence[:(min_len//2)] )\n",
    "#     return '<empty>'\n",
    "\n",
    "# get_sample(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057820ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16660"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "words = sorted( set(itertools.chain(*text)) - set([PAD,EOS]) )\n",
    "word2index = { w:i+2 for i,w in enumerate(words) }\n",
    "del words\n",
    "\n",
    "word2index[PAD]=0\n",
    "word2index[EOS]=1\n",
    "\n",
    "index2word = { i:w for  w,i in word2index.items() }\n",
    "\n",
    "total_words = len(word2index)\n",
    "\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2eb0938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams = [\n",
    "   [ word2index[token] for token in sentence[:i+1] ]\n",
    "   for sentence in text\n",
    "   for i in range(1,len(sentence))\n",
    "]\n",
    "\n",
    "max_sequence_len = max([len(s) for s in n_grams])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad22e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = np.array(pad_sequences(n_grams, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "del n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8596937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3ddaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils as ku \n",
    "inputs, targets = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "targets = ku.np_utils.to_categorical(targets, num_classes=total_words)\n",
    "del input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f60539f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((92194, 112), dtype('int32'), (92194, 16660), dtype('float32'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs.dtype, targets.shape, targets.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bdb92e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa042199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 17:45:04.508519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.570806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.570988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.571429: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-22 17:45:04.571780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.571929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.572051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.934059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.934243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.934369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-22 17:45:04.934477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5054 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 32, input_length=inputs.shape[1]))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(targets.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2bfc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = Embedding(total_words, 16, input_length=inputs.shape[1])\n",
    "# lstm = LSTM(128)\n",
    "# dense = Dense(targets.shape[1], activation='softmax')\n",
    "      \n",
    "# x = inputs[:3]\n",
    "# o = emb(x)\n",
    "# o = lstm(o)\n",
    "# o = dense(o)\n",
    "# o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafb223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddec68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb(inputs[:3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "503780da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 112, 32)           533120    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 112, 128)          82432     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16660)             1082900   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,747,860\n",
      "Trainable params: 1,747,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(model,to_file='cnn.png', show_layer_names=True, show_shapes=True )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62f765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a814006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(inputs,targets,batch_size):\n",
    "    batch_count = np.ceil(len(targets)/batch_size).astype(int)\n",
    "    for i in range(batch_count-1):\n",
    "        yield (\n",
    "            inputs[i*batch_size:(i+1)*batch_size],\n",
    "            targets[i*batch_size:(i+1)*batch_size],\n",
    "        )\n",
    "\n",
    "# g = batch_generator(inputs,targets,batch_size=10)\n",
    "# next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa62405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-22 17:45:09.336980: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    100/Unknown - 14s 112ms/step - loss: 7.8640 - accuracy: 0.1018WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "100/100 [==============================] - 14s 115ms/step - loss: 7.8640 - accuracy: 0.1018\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "100/100 [==============================] - 0s 274us/step - loss: 7.8640 - accuracy: 0.1018\n",
      "CPU times: user 10.2 s, sys: 3.71 s, total: 13.9 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "batch_size = len(targets)//100\n",
    "    \n",
    "# history = model.fit(\n",
    "#     inputs, \n",
    "#     targets, \n",
    "#     epochs=2, \n",
    "#     verbose=1, \n",
    "#     callbacks=[earlystop],\n",
    "#     validation_split=.1,\n",
    "#     batch_size=batch_size,\n",
    "# )\n",
    "\n",
    "# history = model.fit( inputs, targets, epochs=2, verbose=1, batch_size=batch_size,)\n",
    "\n",
    "history = model.fit( \n",
    "    batch_generator(inputs,targets,batch_size=batch_size), \n",
    "    epochs=2, \n",
    "    verbose=1,\n",
    "    callbacks=[earlystop],\n",
    "#     validation_split=.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5034887",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b673608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# score = model.evaluate(x_test, y_test, verbose=False)\n",
    "# print('Test accuracy:', score[1])\n",
    "\n",
    "# history_dict = history.history\n",
    "# history_dict.keys()\n",
    "\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# plt.clf()   # clear figure\n",
    "# acc_values = history_dict['accuracy']\n",
    "# val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "# plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a2ffb",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63aa7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import sent_tokenize as nltk_sentence_split\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "    # basic cleanup\n",
    "    corpus = text = nltk_sentence_split(data)\n",
    "    \n",
    "    # data.lower().split(\"\\n\")\n",
    "\n",
    "    # tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create input sequences using list of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # pad sequences \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.np_utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bda208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_preparation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    model.fit(predictors, label, epochs=100, verbose=1, callbacks=[earlystop])\n",
    "    print( model.summary() )\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12902942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "FILE_DATA = '../data/dostoevsky-besy-p2.txt.gz'\n",
    "with gzip.open(FILE_DATA,'rt',encoding='utf-8') as f: data = f.read()     \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de48907",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, label, max_sequence_len, total_words = dataset_preparation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(predictors, label, max_sequence_len, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from nltk.tokenize import sent_tokenize as nltk_sentence_split\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenize_word\n",
    "\n",
    "text = [ \n",
    "    nltk_tokenize_word(s) # разбиваем предложения на слова\n",
    "    for s in nltk_sentence_split(data) # режем текст на отдельные предложения\n",
    "]\n",
    "\n",
    "def get_sample(text,min_len=10):\n",
    "    for _ in range(100):\n",
    "        sentence = sample(text,1)[0]\n",
    "        # print(len(sentence),sentence)\n",
    "        if len(sentence)>min_len:\n",
    "            return ' '.join(sentence[:(min_len//2)] )\n",
    "    return '<empty>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sentence = get_sample(text) \n",
    "\n",
    "sentence = init_sentence\n",
    "answer = []\n",
    "\n",
    "for _ in range(7):\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted_index = np.argmax( model.predict(token_list, verbose=0) )\n",
    "    predicted_word = tokenizer.index_word[predicted_index]\n",
    "    answer.append(predicted_word)\n",
    "    sentence+=' '+predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ac168",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sentence + ' | ' + ' '.join(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795be6e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40180432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1237e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = text[1]\n",
    "# tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "\n",
    "# token_list[:i+1]\n",
    "# for line in text\n",
    "# for i,token_idx in enumerate( tokenizer.texts_to_sequences([line])[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sequences = []\n",
    "# for line in text:\n",
    "#     token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "#     for i in range(1, len(token_list)):\n",
    "#         n_gram_sequence = token_list[:i+1]\n",
    "#         input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb29e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# max_sequence_len = max([len(x) for x in input_sequences])\n",
    "# input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# # input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1af4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # create predictors and label\n",
    "# predictors, label = input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.utils as ku \n",
    "# label = ku.np_utils.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab24a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# import keras.utils as ku \n",
    "# import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e555017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Tokenizer: Text tokenization utility class.\n",
    "# Functions\n",
    "# hashing_trick(...): Converts a text to a sequence of indexes in a fixed-size hashing space.\n",
    "# one_hot(...): One-hot encodes a text into a list of word indexes of size n.\n",
    "# text_to_word_sequence(...): Converts a text to a sequence of words (or tokens).\n",
    "# tokenizer_from_json(...): Parses a JSON tokenizer configuration file and returns a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text.StateBasedSentenceBreaker \n",
    "# break_sentences(\n",
    "#     doc\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "# tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# class Tokenizer: Text tokenization utility class.\n",
    "# Functions\n",
    "# hashing_trick(...): Converts a text to a sequence of indexes in a fixed-size hashing space.\n",
    "# one_hot(...): One-hot encodes a text into a list of word indexes of size n.\n",
    "# text_to_word_sequence(...): Converts a text to a sequence of words (or tokens).\n",
    "# tokenizer_from_json(...): Parses a JSON tokenizer configuration file and returns a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1892f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample\n",
    "# from nltk.tokenize import sent_tokenize as nltk_sentence_split\n",
    "# from nltk.tokenize import word_tokenize as nltk_tokenize_word\n",
    "\n",
    "# text = [ \n",
    "#     nltk_tokenize_word(s) # разбиваем предложения на слова\n",
    "#     for s in nltk_sentence_split(text) # режем текст на отдельные предложения\n",
    "# ]\n",
    "# print('предложений: %i\\n'%(len(text)))\n",
    "\n",
    "# text = nltk_sentence_split(data)\n",
    "# sample(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602efa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
