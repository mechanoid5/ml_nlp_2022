{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9bfcd8",
   "metadata": {},
   "source": [
    "**Нейросетевая языковая модель**\n",
    "\n",
    "Евгений Борисов <esborisov@sevsu.ru>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35bc3061",
   "metadata": {},
   "source": [
    "+ разрезаем на предложения\n",
    "+ выполняем токенизацию\n",
    "+ строим словарь\n",
    "+ кодируем текст\n",
    "  собираем датасет [контекст слово]\n",
    "  обучаем модель предсказыывать слово по контексту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset getitem\n",
    "# DataLoader generate_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "# from random import sample\n",
    "\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "\n",
    "import torch\n",
    "from torchtext.vocab  import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d58793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        super().__init__()\n",
    "        self._UNK = '<unk>'\n",
    "        self._tokens = self._tokenize(self._load(file_name))\n",
    "        self._vocab = self._build_vocab( tokens = self._tokens,token_default=self._UNK)\n",
    "         \n",
    "    @staticmethod       \n",
    "    def _load(file_name):\n",
    "        with gzip.open(file_name,'rt') as f: text = f.read() \n",
    "        return text\n",
    "      \n",
    "    @staticmethod       \n",
    "    def _tokenize(text):\n",
    "        return [ \n",
    "            [ w.text for w in tokenize(s.text) ] # разбиваем предложения на слова\n",
    "            for s in sentenize(text) # режем текст на отдельные предложения\n",
    "        ]\n",
    "    @staticmethod\n",
    "    def _build_vocab(tokens,token_default):\n",
    "        vocab = build_vocab_from_iterator( tokens, specials=[token_default])\n",
    "        vocab.set_default_index(vocab[token_default])\n",
    "        return vocab\n",
    "    \n",
    "    @property\n",
    "    def vocabulary(self): return self._vocab\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._tokens[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b79c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(TextDataset):\n",
    "\n",
    "    def __init__(self, file_name,context_deep=7,context_deep_min=2):\n",
    "        assert (context_deep>context_deep_min)\n",
    "        \n",
    "        super().__init__(file_name)\n",
    "        self._target = self._flatten_sentences(context_deep_min)\n",
    "        self._contex = self._collect_context(context_deep)\n",
    "        \n",
    "    def _flatten_sentences(self,context_deep_min):\n",
    "        return [ self._vocab[t] for s in self._tokens for t in s[context_deep_min:] ]\n",
    "    \n",
    "    def _collect_context(self,context_deep,context_deep_min):\n",
    "        return [\n",
    "            c\n",
    "            for s in self._tokens\n",
    "            for c in self._collect_context_sentence(s,context_deep,context_deep_min )\n",
    "        ]\n",
    "    \n",
    "    def _collect_context_sentence(self,sentence,context_deep,context_deep_min):\n",
    "        sentence_ = self._sentence_padding(sentence,context_deep)\n",
    "        context_count = len(sentence_)-context_deep\n",
    "        return [ sentence_[i:i+context_deep] for i in range(context_count) ]\n",
    "\n",
    "    def _sentence_padding(self,sentence,context_deep):\n",
    "        pad = [self._UNK]*(context_deep-context_deep_min)\n",
    "        return [ self._vocab[t] for t in ( pad + sentence) ]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            np.array( self._contex[idx], dtype=np.int32 ),\n",
    "            # np.array( self._target[idx], dtype=np.int32 ),\n",
    "            self._target_ohe( self._target[idx] ),\n",
    "        )\n",
    "    \n",
    "    def _target_ohe(self,target):\n",
    "        return np.eye(len(self._vocab),dtype=np.float32)[target]\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self._target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXME: проверка минимальной длинны sentence > context_deep_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ContextDataset('../data/dostoevsky-besy-p2.txt.gz',context_deep=7)\n",
    "vocabulary_len = len(dataset.vocabulary)\n",
    "\n",
    "print('vocabulary size:', vocabulary_len)\n",
    "print('dataset size:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e25801",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ac743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self._emb0 = nn.Embedding(vocab_size,emb_dim)\n",
    "        self._rnn0 = nn.LSTM(emb_dim, hid_dim,)\n",
    "        self._lin0 = nn.Linear(hid_dim,vocab_size) \n",
    "        self._smx0 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        o = self._emb0(x)\n",
    "        o,_ = self._rnn0(o)\n",
    "#         o = o[:,-1,:]\n",
    "        o = self._lin0( o[:,-1,:] )\n",
    "        o = self._smx0(o)\n",
    "        return o\n",
    "        \n",
    "    def predict(self, x):    \n",
    "        return self.forward(x)\n",
    "\n",
    "model = Model(vocab_size=vocabulary_len,emb_dim=(1024*8),hid_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "x, target = next(iter(  DataLoader( dataset, batch_size=64, shuffle=True ) ))\n",
    "predicted = model.predict(x)\n",
    "\n",
    "x.shape, predicted.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83aaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b26ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.BCELoss()(predicted,target)\n",
    "# nn.MSELoss()(predicted,target)\n",
    "# nn.CrossEntropyLoss()(predicted,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99155441",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем GPU если есть\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "# target    = []\n",
    "# predicted = []\n",
    "\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     for x,y in tqdm(train_dataloader):\n",
    "#         predicted.append( np.argmax( model.predict(x.to(device)).cpu().numpy(),axis=1 ) )\n",
    "#         target.append(y.numpy())\n",
    "        \n",
    "# accuracy_score( np.hstack( target ), np.hstack( predicted ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f447f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def accuracy(x,target):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         predicted = model.predict(x.to(device)).cpu().numpy()\n",
    "#     return accuracy_score(np.argmax(target.numpy(),axis=1),np.argmax(predicted,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e486cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     for x,target in tqdm(test_dataloader):\n",
    "#         acc_history = [ accuracy(x,target) ] # начальное значение погрешности\n",
    "\n",
    "# acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04118a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ф-ция потери\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# метод оптимизации ф-ции потери\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-2) \n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.set_grad_enabled(False):\n",
    "#     loss = criterion( \n",
    "#             torch.Tensor(y_train).to(device), \n",
    "#             model.predict( torch.Tensor(X_train).to(device) ) \n",
    "#         ).cpu().numpy().flatten()[0]\n",
    "    \n",
    "# loss_history = [ loss ] # начальное значение ф-ции потери\n",
    "\n",
    "# loss_history = [] # начальное значение ф-ции потери"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(predicted,target):\n",
    "    return accuracy_score(\n",
    "        np.argmax(target.cpu().numpy(),axis=1),\n",
    "        np.argmax(predicted.detach().cpu().numpy(),axis=1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.notebook import tqdm # рисует прогрессбар\n",
    "from torch.utils.data import DataLoader # генератор батчей\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "# acc_min = .98 # порог минимально допустимой погрешности модели\n",
    "\n",
    "n_epoch = 2 # количество эпох обучения\n",
    "\n",
    "# epoch = tqdm(range(n_epoch))\n",
    "\n",
    "for epoch in range(n_epoch): \n",
    "    \n",
    "    batches = tqdm(DataLoader( dataset, batch_size=256, shuffle=True ) )\n",
    "    \n",
    "    for x,target in batches:\n",
    "        o = model.forward( x.to(device) ) # считаем выход модели\n",
    "        loss = criterion( target.to(device),o ) # вычисляем значение ф-ции потери\n",
    "        loss_history.append(loss.item()) # дополняем историю изменения значений ф-ции потери\n",
    "        optimizer.zero_grad() # очищаем предыдущее значение градиента\n",
    "        loss.backward()  # вычисляем текущее значение градиента ф-ции потери\n",
    "        optimizer.step() # корректируем параметры модели\n",
    "        acc_history.append( accuracy(o,target) ) #значение погрешности\n",
    "\n",
    "        batches.set_postfix({\n",
    "            'loss':loss_history[-1], \n",
    "             'acc':acc_history[-1],\n",
    "        })\n",
    "        \n",
    "    #if acc_history[-1] > acc_min: # проверяем достижение минимального порога погрешности модели\n",
    "    #    print('step %i/%i: loss %.03f, acc threshold %.03f reached\\n'%(i+1,n_epoch,loss_history[-1],acc_min))\n",
    "    #    break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9343870",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(batch_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acff0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# em0 = nn.Embedding(vocabulary_len,1024)\n",
    "# rc0  = nn.LSTM(1024, 32,)\n",
    "# dense0 = nn.Linear(32,vocabulary_len) \n",
    "# smax0 = nn.Softmax(dim=1)\n",
    "\n",
    "# o = smax0( dense0( rc0( em0(batch_input) )[0][:,-1,:] ) )\n",
    "\n",
    "# o.shape\n",
    "\n",
    "# # o[:,-1,:].shape, len(s), s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smax1 = nn.Softmax(dim=1)\n",
    "# x = torch.randn(2, 3)\n",
    "# smax1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor( batch_input ) #, batch_target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_input \n",
    "# batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMTagger(nn.Module):\n",
    "\n",
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "#         super(LSTMTagger, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#         # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "#         # with dimensionality hidden_dim.\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "#         # The linear layer that maps from hidden state space to tag space\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "#     def forward(self, sentence):\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5835e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch import sigmoid\n",
    "\n",
    "# class MLP(nn.Module): \n",
    "    \n",
    "#     def __init__(self,input_size,output_size):\n",
    "#         super().__init__()\n",
    "#         self.dense1 = nn.Linear(input_size,10) # первый - обрабатывающий  слой \n",
    "#         self.dense2 = nn.Linear(10,5) # второй - обрабатывающий/скрытый слой\n",
    "#         self.dense3 = nn.Linear(5,output_size) # третий - обрабатывающий/выходной слой\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         o = sigmoid(self.dense1(x))\n",
    "#         o = sigmoid(self.dense2(o))\n",
    "#         o = sigmoid(self.dense3(o))\n",
    "#         return o\n",
    "    \n",
    "#     def predict(self, x):    \n",
    "#         return self.forward(x)\n",
    "    \n",
    "    \n",
    "# model = MLP( input_size=X_train.shape[1], output_size=y_train.shape[1], )\n",
    "\n",
    "# from torch import optim\n",
    "\n",
    "# criterion = nn.MSELoss() # ф-ция потери\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-2) # метод оптимизации ф-ции потери\n",
    "\n",
    "\n",
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# # пакуем данные в формат Torch\n",
    "# dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train) )\n",
    "\n",
    "\n",
    "# # используем GPU если есть\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def accuracy(x,y):\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         o = np.argmax( model.predict(torch.Tensor(x).to(device)).cpu().numpy(),axis=1 )\n",
    "#     return accuracy_score(y[:,1],o)\n",
    "\n",
    "# acc_history = [ accuracy(X_train,y_train) ] # начальное значение погрешности\n",
    "\n",
    "\n",
    "\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     loss = criterion( \n",
    "#             torch.Tensor(y_train).to(device), \n",
    "#             model.predict( torch.Tensor(X_train).to(device) ) \n",
    "#         ).cpu().numpy().flatten()[0]\n",
    "    \n",
    "# loss_history = [ loss ] # начальное значение ф-ции потери\n",
    "\n",
    "\n",
    "# %%time\n",
    "\n",
    "# from tqdm import tqdm # рисует прогрессбар\n",
    "# from torch.utils.data import DataLoader # генератор батчей\n",
    "\n",
    "# n_epoch = 500 # количество эпох обучения\n",
    "# acc_min = .98 # порог минимально допустимой погрешности модели\n",
    "\n",
    "# for i in tqdm(range(n_epoch)): \n",
    "    \n",
    "#     for x,y in DataLoader(dataset, batch_size=len(y_train)//3, shuffle=True): # получаем батч учебных примеров\n",
    "#         out = model.forward(x.to(device)) # считаем выход модели\n",
    "#         loss = criterion( y.to(device),out ) # вычисляем значение ф-ции потери\n",
    "#         loss_history.append(loss.item()) # дополняем историю изменения значений ф-ции потери\n",
    "#         optimizer.zero_grad() # очищаем предыдущее значение градиента\n",
    "#         loss.backward()  # вычисляем текущее значение градиента ф-ции потери\n",
    "#         optimizer.step() # корректируем параметры модели\n",
    "        \n",
    "#     acc_history.append( accuracy(X_train,y_train) ) #значение погрешности\n",
    "#     if acc_history[-1] > acc_min: # проверяем достижение минимального порога погрешности модели\n",
    "#         print('step %i/%i: loss %.03f, acc threshold %.03f reached\\n'%(i+1,n_epoch,loss_history[-1],acc_min))\n",
    "#         break\n",
    "        \n",
    "        \n",
    "# # история изменения значений погрешности модели\n",
    "# plt.plot(acc_history,label='max acc=%.3f'%(max(acc_history)),c='r')\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# # история изменения значений ф-ции потери\n",
    "# plt.plot(loss_history,label='min loss=%.3f'%(min(loss_history)))\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     s = model.predict( torch.Tensor(X_test).to(device)).cpu().numpy()[:,1]\n",
    "    \n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve( y_test[:,1], s )\n",
    "# roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# plt.figure(figsize=(7,7))\n",
    "# plt.grid(True)\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC AUC %0.2f' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea278199",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8633b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def __len__(self):\n",
    "#         return len(self._tokens)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = [\n",
    "#  'Я-то',\n",
    "#  'кой-куда',\n",
    "#  'еще',\n",
    "#  'выходил',\n",
    "#  'и',\n",
    "#  'по-прежнему',\n",
    "#  'приносил',\n",
    "#  'ему',\n",
    "#  'разные',\n",
    "#  'вести',\n",
    "#  ',',\n",
    "#  'без',\n",
    "#  'чего',\n",
    "#  'он',\n",
    "#  'и',\n",
    "#  'пробыть',\n",
    "#  'не',\n",
    "#  'мог',\n",
    "#  '.',\n",
    "# ]\n",
    "\n",
    "# context_deep = 3\n",
    "\n",
    "# UNK_KWD = '<unk>'\n",
    "\n",
    "# sentence_ = [UNK_KWD,]*context_deep + sentence\n",
    "\n",
    "# [ \n",
    "#  [ sentence_[i:i+context_deep] ]  \n",
    "#  for i in range(len(sentence)) \n",
    "# ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data)\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb091ab1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f076b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['отворил'],vocab['сам'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchtext\n",
    "# from torchtext.data import Field\n",
    "# from torchtext.data import BucketIterator\n",
    "# from torchtext.data import TabularDataset\n",
    "\n",
    "# en = spacy.load('en')\n",
    "# fr = spacy.load('fr')\n",
    "\n",
    "# def tokenize_en(sentence):\n",
    "#     return [tok.text for tok in en.tokenizer(sentence)]\n",
    "\n",
    "# def tokenize_fr(sentence):\n",
    "#     return [tok.text for tok in fr.tokenizer(sentence)]\n",
    "\n",
    "# EN_TEXT = Field(tokenize=tokenize_en)\n",
    "# FR_TEXT = Field(tokenize=tokenize_fr, init_token = \"<sos>\", eos_token = \"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ada4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(voc)\n",
    "# [ w for w in voc ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d169b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from nltk.util import bigrams\n",
    "# from nltk.util import ngrams as nltk_ngrams\n",
    "\n",
    "# # вынимаем все n-gram из текста\n",
    "# ngram_len = 3 # работаем с триграммами\n",
    "# text_ngrams = [ ngram for s in text for ngram in nltk_ngrams(s,ngram_len) ]\n",
    "# print('количество n-gram: %i'%(len(set(text_ngrams))))\n",
    "# sample(text_ngrams,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983694d4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.util import flatten as nltk_flatten\n",
    "\n",
    "# vocab = { w:i for i,w in enumerate(sorted(set(nltk_flatten(text)))) }\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ \n",
    "#     [ vocab[w] for w in t ]\n",
    "#     for t in text \n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf7411",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# def yield_tokens(data_iter):\n",
    "#     for _, text in data_iter:\n",
    "#         yield tokenizer(text)\n",
    "\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7b44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b49422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import gzip\n",
    "\n",
    "# # загружаем текст ...\n",
    "# file_name = '../data/dostoevsky-besy-p2.txt.gz'\n",
    "# with gzip.open(file_name,'rt') as f:  \n",
    "#     text = f.read()[105:] # ...и выкидываем заголовок\n",
    "\n",
    "# print('символов:%i\\n'%(len(text)))\n",
    "# print(text[:364].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample\n",
    "\n",
    "# from razdel import sentenize\n",
    "# from razdel import tokenize\n",
    "\n",
    "# tokens = [ \n",
    "#     [ w.text for w in tokenize(s.text) ] # разбиваем предложения на слова\n",
    "#     for s in sentenize(text) # режем текст на отдельные предложения\n",
    "# ]\n",
    "\n",
    "# print('предложений: %i\\n'%(len(tokens)))\n",
    "\n",
    "# sample(tokens,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335b774",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# vocab = build_vocab_from_iterator(tokens, specials=['<unk>',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80be9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ vocab[t] for t in tokens[1] ]\n",
    "# vocab['<unk>']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
