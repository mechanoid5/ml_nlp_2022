{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9bfcd8",
   "metadata": {},
   "source": [
    "**Нейросетевая языковая модель**\n",
    "\n",
    "Евгений Борисов <esborisov@sevsu.ru>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import gzip\n",
    "\n",
    "# # загружаем текст ...\n",
    "# file_name = '../data/dostoevsky-besy-p2.txt.gz'\n",
    "# with gzip.open(file_name,'rt') as f:  \n",
    "#     text = f.read()[105:] # ...и выкидываем заголовок\n",
    "\n",
    "# print('символов:%i\\n'%(len(text)))\n",
    "# print(text[:364].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d03d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import sample\n",
    "\n",
    "# from razdel import sentenize\n",
    "# from razdel import tokenize\n",
    "\n",
    "# tokens = [ \n",
    "#     [ w.text for w in tokenize(s.text) ] # разбиваем предложения на слова\n",
    "#     for s in sentenize(text) # режем текст на отдельные предложения\n",
    "# ]\n",
    "\n",
    "# print('предложений: %i\\n'%(len(tokens)))\n",
    "\n",
    "# sample(tokens,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0fd95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c455a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# vocab = build_vocab_from_iterator(tokens, specials=['<unk>',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33fa9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ vocab[t] for t in tokens[1] ]\n",
    "# vocab['<unk>']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35bc3061",
   "metadata": {},
   "source": [
    "+ разрезаем на предложения\n",
    "+ выполняем токенизацию\n",
    "+ строим словарь\n",
    "+ кодируем текст\n",
    "  собираем датасет [контекст слово]\n",
    "  обучаем модель предсказыывать слово по контексту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2178d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset getitem\n",
    "# DataLoader generate_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d58793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "# from random import sample\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_name):\n",
    "        super().__init__()\n",
    "        self._UNK = '<unk>'\n",
    "        self._data = self._tokenize(self._load(file_name))\n",
    "        self._vocab = self._build_vocab( tokens = self._data,token_default=self._UNK)\n",
    "         \n",
    "    @staticmethod       \n",
    "    def _load(file_name):\n",
    "        with gzip.open(file_name,'rt') as f: text = f.read() \n",
    "        return text\n",
    "      \n",
    "    @staticmethod       \n",
    "    def _tokenize(text):\n",
    "        return [ \n",
    "            [ w.text for w in tokenize(s.text) ] # разбиваем предложения на слова\n",
    "            for s in sentenize(text) # режем текст на отдельные предложения\n",
    "        ]\n",
    "    @staticmethod\n",
    "    def _build_vocab(tokens,token_default):\n",
    "        vocab = build_vocab_from_iterator( tokens, specials=[token_default])\n",
    "        vocab.set_default_index(vocab[token_default])\n",
    "        return vocab\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b79c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(TextDataset):\n",
    "\n",
    "    def __init__(self, file_name,context_deep=3):\n",
    "        super().__init__(file_name)\n",
    "        self._context_deep = context_deep\n",
    "        self._contex = self._collect_context()\n",
    "        self._tokens = self._flatten_sentences()\n",
    "        \n",
    "    def _flatten_sentences(self,):\n",
    "        return [\n",
    "            self._vocab[t]\n",
    "            for s in self._data\n",
    "            for t in s\n",
    "        ]\n",
    "    \n",
    "    def _collect_context(self,):\n",
    "        return [\n",
    "            c\n",
    "            for s in self._data\n",
    "            for c in self._collect_context_sentence( self._encode_tokens(s) )\n",
    "        ]\n",
    "        \n",
    "    def _encode_tokens(self,tokens):\n",
    "        return [ self._vocab[t] for t in ([self._UNK]*self._context_deep + tokens) ]\n",
    "\n",
    "    \n",
    "    def _collect_context_sentence(self,sentence):\n",
    "        return [ \n",
    "            sentence[i:i+self._context_deep]  \n",
    "            for i in range(len(sentence)-self._context_deep) \n",
    "        ]\n",
    "     \n",
    "    def __getitem__(self, idx):\n",
    "        return self._contex[idx],self._tokens[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fd4d732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6101, 3618, 3521], 6307)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ContextDataset('../data/dostoevsky-besy-p2.txt.gz')\n",
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b2a4222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([  102,     0,     9,     0,     1, 11270,     1,    41,     1,     0,\n",
       "              5,    56,  5064,   214,    10,   337,     5,     0, 16294,  2498,\n",
       "            260,     0,   167,   714,   991,   704,     0,     1,     0,     7,\n",
       "              5,     3,     1,     3,  5592,     0,    10,     5,  1799,     1,\n",
       "              7,     4,    47,  1510,     0,    72,  4018,     0,   709,    19,\n",
       "             59,    13,   384,    11,    20,     0, 14405,     5,  6780,     0,\n",
       "              3,   164,   204,  1491]),\n",
       "  tensor([ 9381,     0,    78,    79,  4478, 12258,   302,    97,     7,    22,\n",
       "             10,    18,  1509,    53,  3002,   378, 11849,     0,    13,  2520,\n",
       "              7,     0,    32,    37,  3714,   937,   202,     3,     3,    12,\n",
       "          16486,    59,    92,    79,     9,     0,   474,   890,    20,    37,\n",
       "            132,   272,    24,    25,   147,   216,    13,     0,    51,   197,\n",
       "            222,     5,  4104,    67,    47,     0,     1, 12382,    33,     3,\n",
       "            283,     6,    98, 16048]),\n",
       "  tensor([   26,    59,     1,   125,     4,     1,     1,    87,   322,   536,\n",
       "            457,    56,  1518,    55,    45,     4,    45,    50,   285,   236,\n",
       "           8823,     0,     5, 15065,  1804,   941,   430, 11220,  5903,    74,\n",
       "             72,   131,    43,    14,    27,     0,    39,  4784,    25,    26,\n",
       "            137,    25,  1261,  8209,    20,    31,    84,     0,   135,   834,\n",
       "             26,  3270,  6055,  1201,   273,     3,    21,  5619,   474,   283,\n",
       "              1,  1021,   494,   339])],\n",
       " tensor([  358,   126,   170,   110,    23,     4,    11,  5000,     1,     1,\n",
       "            10,     1,     1,     1,   224,  3853,    68,   310,   327,     8,\n",
       "             1,     3,  7529,     1,   138,     2,  1003,  1960,     1,    52,\n",
       "            36,     5,     1,    86,   321,  2370,    30,     2,   458,  4175,\n",
       "           234,   126,  1244,  2684,  1843,    38,     1,     3,   321, 10414,\n",
       "             4,   310,  5966,     1,   284,  1879,    35,  4861,    46,     1,\n",
       "             3, 10267,   294,  9724])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(data, batch_size=64, shuffle=True)\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8633b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self._tokens)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = [\n",
    "#  'Я-то',\n",
    "#  'кой-куда',\n",
    "#  'еще',\n",
    "#  'выходил',\n",
    "#  'и',\n",
    "#  'по-прежнему',\n",
    "#  'приносил',\n",
    "#  'ему',\n",
    "#  'разные',\n",
    "#  'вести',\n",
    "#  ',',\n",
    "#  'без',\n",
    "#  'чего',\n",
    "#  'он',\n",
    "#  'и',\n",
    "#  'пробыть',\n",
    "#  'не',\n",
    "#  'мог',\n",
    "#  '.',\n",
    "# ]\n",
    "\n",
    "# context_deep = 3\n",
    "\n",
    "# UNK_KWD = '<unk>'\n",
    "\n",
    "# sentence_ = [UNK_KWD,]*context_deep + sentence\n",
    "\n",
    "# [ \n",
    "#  [ sentence_[i:i+context_deep] ]  \n",
    "#  for i in range(len(sentence)) \n",
    "# ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data)\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb091ab1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f076b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['отворил'],vocab['сам'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchtext\n",
    "# from torchtext.data import Field\n",
    "# from torchtext.data import BucketIterator\n",
    "# from torchtext.data import TabularDataset\n",
    "\n",
    "# en = spacy.load('en')\n",
    "# fr = spacy.load('fr')\n",
    "\n",
    "# def tokenize_en(sentence):\n",
    "#     return [tok.text for tok in en.tokenizer(sentence)]\n",
    "\n",
    "# def tokenize_fr(sentence):\n",
    "#     return [tok.text for tok in fr.tokenizer(sentence)]\n",
    "\n",
    "# EN_TEXT = Field(tokenize=tokenize_en)\n",
    "# FR_TEXT = Field(tokenize=tokenize_fr, init_token = \"<sos>\", eos_token = \"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ada4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(voc)\n",
    "# [ w for w in voc ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d169b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from nltk.util import bigrams\n",
    "# from nltk.util import ngrams as nltk_ngrams\n",
    "\n",
    "# # вынимаем все n-gram из текста\n",
    "# ngram_len = 3 # работаем с триграммами\n",
    "# text_ngrams = [ ngram for s in text for ngram in nltk_ngrams(s,ngram_len) ]\n",
    "# print('количество n-gram: %i'%(len(set(text_ngrams))))\n",
    "# sample(text_ngrams,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983694d4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.util import flatten as nltk_flatten\n",
    "\n",
    "# vocab = { w:i for i,w in enumerate(sorted(set(nltk_flatten(text)))) }\n",
    "# print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ \n",
    "#     [ vocab[w] for w in t ]\n",
    "#     for t in text \n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf7411",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# train_iter = AG_NEWS(split='train')\n",
    "\n",
    "# def yield_tokens(data_iter):\n",
    "#     for _, text in data_iter:\n",
    "#         yield tokenizer(text)\n",
    "\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7b44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
